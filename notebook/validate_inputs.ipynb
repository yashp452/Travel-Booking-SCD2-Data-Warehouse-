{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af01291c-fb79-43ab-9209-1ae68d670f10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "try:\n",
    "    arrival_date=dbutils.widgets.get(\"arrival_date\")\n",
    "except:\n",
    "    arrival_date=dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "try:\n",
    "    catalog=dbutils.widgets.get('catalog')\n",
    "except Exception:\n",
    "    catalog=\"travel_bookings\"\n",
    "\n",
    "try:\n",
    "    schema=dbutils.widgets.get('schema')\n",
    "except Exception:\n",
    "    schema=\"default\"\n",
    "\n",
    "try:\n",
    "    base_volume=dbutils.widgest.get('base_volume')\n",
    "except Exception:\n",
    "    base_volume=f\"/Volume/{catalog}/{schema}/data\"\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp,lit,to_date\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "booking_path=f\"{base_volume}/booking_data/bookings_{arrival_date}.csv\"\n",
    "customer_path=f\"{base_volume}/customer_data/customers_{arrival_date}.csv\"\n",
    "\n",
    "missing = []\n",
    "try:\n",
    "    dbutils.fs.ls(booking_path)\n",
    "except Exception:\n",
    "    missing.append(booking_path)\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(customer_path)\n",
    "except Exception:\n",
    "    missing.append(customer_path)\n",
    "\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing input files: {missing}\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.ops\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.ops.run_log (\n",
    "  run_id STRING,\n",
    "  arrival_date DATE,\n",
    "  stage STRING,\n",
    "  status STRING,\n",
    "  message STRING,\n",
    "  recorded_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "run_id = f\"nb-validate-{arrival_date}-{int(time.time())}\"\n",
    "log_df = spark.createDataFrame([\n",
    "    (run_id, arrival_date, \"validate_inputs\", \"STARTED\", \"Inputs validated\")\n",
    "], [\"run_id\",\"arrival_date\",\"stage\",\"status\",\"message\"])\n",
    "log_df = log_df.withColumn(\"arrival_date\", F.to_date(\"arrival_date\")).withColumn(\"recorded_at\", current_timestamp())\n",
    "log_df.write.mode(\"append\").saveAsTable(f\"{catalog}.ops.run_log\")\n",
    "\n",
    "print(\"Validation successful:\", booking_path, customer_path)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "100db624-9222-4f06-819f-a60aad73c875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "validate_inputs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
