{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7309a0c9-aca3-497c-8d73-814e21c12d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "import datetime as _dt\n",
    "try:\n",
    "    arrival_date = dbutils.widgets.get(\"arrival_date\")\n",
    "except Exception:\n",
    "    arrival_date = _dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "try:\n",
    "    catalog = dbutils.widgets.get(\"catalog\")\n",
    "except Exception:\n",
    "    catalog = \"travel_bookings\"\n",
    "try:\n",
    "    schema = dbutils.widgets.get(\"schema\")\n",
    "except Exception:\n",
    "    schema = \"default\"\n",
    "\n",
    "# =============================================================================\n",
    "# SOURCE DATA PREPARATION\n",
    "# =============================================================================\n",
    "# Load customer data from bronze layer for the specified business date\n",
    "# Filters to current day's data for incremental DQ processing\n",
    "\n",
    "src = spark.table(f\"{catalog}.bronze.customer_inc\").where(F.col(\"business_date\") == F.to_date(F.lit(arrival_date)))\n",
    "\n",
    "# =============================================================================\n",
    "# DATA QUALITY CHECKS DEFINITION\n",
    "# =============================================================================\n",
    "# Define comprehensive DQ checks using PyDeequ framework\n",
    "# hasSize: Ensures data exists (row count > 0)\n",
    "# isComplete: Validates required customer fields are not null\n",
    "# Focus on customer-specific attributes: name, address, email\n",
    "\n",
    "check = (Check(spark, CheckLevel.Error, \"Customer Data Check\")\n",
    "         .hasSize(lambda x: x > 0)\n",
    "         .isComplete(\"customer_name\")\n",
    "         .isComplete(\"customer_address\")\n",
    "         .isComplete(\"email\"))\n",
    "\n",
    "# =============================================================================\n",
    "# DQ EXECUTION AND RESULTS\n",
    "# =============================================================================\n",
    "# Execute DQ checks and capture results for audit logging\n",
    "# Displays results for immediate review and stores for historical tracking\n",
    "\n",
    "result = (VerificationSuite(spark).onData(src).addCheck(check).run())\n",
    "df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "\n",
    "# =============================================================================\n",
    "# DQ RESULTS STORAGE SETUP\n",
    "# =============================================================================\n",
    "# Create operations schema and DQ results table for audit tracking\n",
    "# Stores DQ check results with metadata for monitoring and reporting\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.ops\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.ops.dq_results (\n",
    "  business_date DATE,\n",
    "  dataset STRING,\n",
    "  check_name STRING,\n",
    "  status STRING,\n",
    "  constraint STRING,\n",
    "  message STRING,\n",
    "  recorded_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# DQ RESULTS LOGGING\n",
    "# =============================================================================\n",
    "# Transform and store DQ results with metadata for audit trail\n",
    "# Includes business_date, dataset name, and timestamp for tracking\n",
    "\n",
    "out = (df\n",
    "  .withColumn(\"business_date\", F.to_date(F.lit(arrival_date)))\n",
    "  .withColumn(\"dataset\", F.lit(\"customer_inc\"))\n",
    "  .withColumn(\"recorded_at\", F.current_timestamp()))\n",
    "\n",
    "display(df)\n",
    "\n",
    "out.select(\"business_date\",\"dataset\",\"check\",\"check_status\",\"constraint\",\"constraint_status\",\"constraint_message\",\"recorded_at\") \\\n",
    "   .write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog}.ops.dq_results\")\n",
    "\n",
    "\n",
    "if result.status != \"Success\":\n",
    "  raise ValueError(\"DQ failed for customers\")\n",
    "\n",
    "print(\"Customer DQ passed\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dq_customers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
